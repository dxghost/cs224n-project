{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ff98a4-51e3-4144-bc7a-3b90d582fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from collections import Counter\n",
    "from cleantext import clean\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "source": [
    "### Create Corpus\n",
    "By concatenating all articles, we achieve huge corpus for both football and basketball news"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Corpus Create] basketball news 0.01%\n",
      "[Corpus Create] basketball news 10.01%\n",
      "[Corpus Create] basketball news 20.02%\n",
      "[Corpus Create] basketball news 30.02%\n",
      "[Corpus Create] basketball news 40.03%\n",
      "[Corpus Create] basketball news 50.03%\n",
      "[Corpus Create] basketball news 60.03%\n",
      "[Corpus Create] basketball news 70.04%\n",
      "[Corpus Create] basketball news 80.04%\n",
      "[Corpus Create] basketball news 90.05%\n",
      "[Corpus Create] football news 0.00%\n",
      "[Corpus Create] football news 1.91%\n",
      "[Corpus Create] football news 3.83%\n",
      "[Corpus Create] football news 5.74%\n",
      "[Corpus Create] football news 7.65%\n",
      "[Corpus Create] football news 9.56%\n",
      "[Corpus Create] football news 11.47%\n",
      "[Corpus Create] football news 13.38%\n",
      "[Corpus Create] football news 15.30%\n",
      "[Corpus Create] football news 17.21%\n",
      "[Corpus Create] football news 19.12%\n",
      "[Corpus Create] football news 21.03%\n",
      "[Corpus Create] football news 22.94%\n",
      "[Corpus Create] football news 24.86%\n",
      "[Corpus Create] football news 26.77%\n",
      "[Corpus Create] football news 28.68%\n",
      "[Corpus Create] football news 30.59%\n",
      "[Corpus Create] football news 32.50%\n",
      "[Corpus Create] football news 34.42%\n",
      "[Corpus Create] football news 36.33%\n",
      "[Corpus Create] football news 38.24%\n",
      "[Corpus Create] football news 40.15%\n",
      "[Corpus Create] football news 42.06%\n",
      "[Corpus Create] football news 43.97%\n",
      "[Corpus Create] football news 45.89%\n",
      "[Corpus Create] football news 47.80%\n",
      "[Corpus Create] football news 49.71%\n",
      "[Corpus Create] football news 51.62%\n",
      "[Corpus Create] football news 53.53%\n",
      "[Corpus Create] football news 55.45%\n",
      "[Corpus Create] football news 57.36%\n",
      "[Corpus Create] football news 59.27%\n",
      "[Corpus Create] football news 61.18%\n",
      "[Corpus Create] football news 63.09%\n",
      "[Corpus Create] football news 65.01%\n",
      "[Corpus Create] football news 66.92%\n",
      "[Corpus Create] football news 68.83%\n",
      "[Corpus Create] football news 70.74%\n",
      "[Corpus Create] football news 72.65%\n",
      "[Corpus Create] football news 74.56%\n",
      "[Corpus Create] football news 76.48%\n",
      "[Corpus Create] football news 78.39%\n",
      "[Corpus Create] football news 80.30%\n",
      "[Corpus Create] football news 82.21%\n",
      "[Corpus Create] football news 84.12%\n",
      "[Corpus Create] football news 86.04%\n",
      "[Corpus Create] football news 87.95%\n",
      "[Corpus Create] football news 89.86%\n",
      "[Corpus Create] football news 91.77%\n",
      "[Corpus Create] football news 93.68%\n",
      "[Corpus Create] football news 95.60%\n",
      "[Corpus Create] football news 97.51%\n",
      "[Corpus Create] football news 99.42%\n"
     ]
    }
   ],
   "source": [
    "bdf = pd.read_json(\"clean/basketball-stem-lem.jl\", lines = True)\n",
    "fdf = pd.read_json(\"clean/football-stem-lem.jl\", lines = True)\n",
    "\n",
    "def create_corpus(df, name):\n",
    "    corpus = \"\"\n",
    "    for i, row in df.iterrows():\n",
    "        if i%1000==0:\n",
    "            print(\"[Corpus Create] {} {:.2f}%\".format(name,((i+1)/len(df))*100))\n",
    "        corpus+=row[\"content\"]\n",
    "        corpus+=\" \"\n",
    "    return corpus\n",
    "\n",
    "b_corpus = create_corpus(bdf,\"basketball news\")\n",
    "f_corpus = create_corpus(fdf,\"football news\")\n"
   ]
  },
  {
   "source": [
    "### Sentence tokenization\n",
    "nltk provides `sent_tokenize` which I used in order to get sentence count for both corpora"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "football_sentences_count:  797716\nbasketball_sentences_count:  315544\n"
     ]
    }
   ],
   "source": [
    "f_sentence_tokens = nltk.tokenize.sent_tokenize(f_corpus)\n",
    "b_sentence_tokens = nltk.tokenize.sent_tokenize(b_corpus)\n",
    "\n",
    "print(\"football_sentences_count: \",len(f_sentence_tokens))\n",
    "print(\"basketball_sentences_count: \",len(b_sentence_tokens))"
   ]
  },
  {
   "source": [
    "### Data Cleaning\n",
    "We don't want punctuation marks to be counted. also after reading data from `.jl` files, fixing unicode would help"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_corpus = clean(f_corpus,\n",
    "            fix_unicode=True,               # fix various unicode errors\n",
    "            to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "            lower=False,                     # lowercase text\n",
    "            no_line_breaks=False,            # fully strip line breaks as opposed to only normalizing them\n",
    "            no_urls=False,                   # replace all URLs with a special token\n",
    "            no_emails=False,                # replace all email addresses with a special token\n",
    "            no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "            no_numbers=False,                # replace all numbers with a special token\n",
    "            no_digits=False,                 # replace all digits with a special token\n",
    "            no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "            no_punct=True,                  # remove punctuations\n",
    "            replace_with_punct=\"\",\n",
    "            lang=\"en\"\n",
    "        )\n",
    "b_corpus = clean(b_corpus,\n",
    "            fix_unicode=True,               # fix various unicode errors\n",
    "            to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "            lower=False,                     # lowercase text\n",
    "            no_line_breaks=False,            # fully strip line breaks as opposed to only normalizing them\n",
    "            no_urls=False,                   # replace all URLs with a special token\n",
    "            no_emails=False,                # replace all email addresses with a special token\n",
    "            no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "            no_numbers=False,                # replace all numbers with a special token\n",
    "            no_digits=False,                 # replace all digits with a special token\n",
    "            no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "            no_punct=True,                  # remove punctuations\n",
    "            replace_with_punct=\"\",\n",
    "            lang=\"en\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"corpus\",\"basketball_corpus.txt\"),\"wb\") as basketball_corpus:\n",
    "    basketball_corpus.write(b_corpus.encode(\"utf-8\"))\n",
    "\n",
    "with open(os.path.join(\"corpus\",\"football_corpus.txt\"),\"wb\") as football_corpus:\n",
    "    football_corpus.write(f_corpus.encode(\"utf-8\"))"
   ]
  },
  {
   "source": [
    "### Remove Number & URL tokens\n",
    "All URLs and numbers were replaced to special tokens in data-cleaning process, we remove them as we don't need to take them into account while counting."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_corpus = f_corpus.replace(\"<number>\",\"\")\n",
    "f_corpus = f_corpus.replace(\"<url>\",\"\")\n",
    "b_corpus = b_corpus.replace(\"<number>\",\"\")\n",
    "b_corpus = b_corpus.replace(\"<url>\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tokens = nltk.word_tokenize(f_corpus)\n",
    "b_tokens = nltk.word_tokenize(b_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "football_words_count:  10005565\nbasketball_words_count:  3500059\nfootball_unique_words_count:  58969\nbasketball_unique_words_count:  26278\ncommon_words_count:  15745\nfootball_different_words_count:  43224\nbasketball_different_words_count:  10533\nfootball_articles_count:  52305\nbasketball_articles_count:  9996\n"
     ]
    }
   ],
   "source": [
    "print(\"football_words_count: \", len(f_tokens))\n",
    "print(\"basketball_words_count: \", len(b_tokens))\n",
    "\n",
    "f_tokens_set = set(f_tokens)\n",
    "b_tokens_set = set(b_tokens)\n",
    "\n",
    "print(\"football_unique_words_count: \", len(f_tokens_set))\n",
    "print(\"basketball_unique_words_count: \", len(b_tokens_set))\n",
    "\n",
    "print(\"common_words_count: \", len(f_tokens_set-(f_tokens_set-b_tokens_set)))\n",
    "print(\"football_different_words_count: \", len(f_tokens_set-b_tokens_set))\n",
    "print(\"basketball_different_words_count: \", len(b_tokens_set-f_tokens_set))\n",
    "\n",
    "print(\"football_articles_count: \", len(fdf))\n",
    "print(\"basketball_articles_count: \", len(bdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('talksport', 22174), ('tottenham', 18933), ('mourinho', 12027), ('everton', 10548), ('klopp', 9603), ('ps00million', 8185), ('goalkeep', 8163), ('ps00', 7679), ('newcastl', 7518), ('solskjaer', 7113), ('trafford', 7059), ('guardiola', 6884)]\n",
      "[('sixer', 2858), ('antetokounmpo', 2739), ('cav', 2637), ('postseason', 2347), ('pacer', 2249), ('doncic', 1943), ('apg', 1873), ('kawhi', 1718), ('lillard', 1711), ('timberwolv', 1500), ('nbacom', 1342), ('kyri', 1238)]\n"
     ]
    }
   ],
   "source": [
    "common_words_set = f_tokens_set-(f_tokens_set-b_tokens_set)\n",
    "f_uncummon_words = [word for word in f_tokens if word not in common_words_set]\n",
    "c = Counter(f_uncummon_words)\n",
    "print(c.most_common(12))\n",
    "\n",
    "b_uncummon_words = [word for word in b_tokens if word not in common_words_set]\n",
    "d = Counter(b_uncummon_words)\n",
    "print(d.most_common(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_union = f_tokens + b_tokens\n",
    "top_ten_common_words = Counter([w for  w in words_union if w in common_words_set]).most_common(10)\n",
    "football_words_count = Counter(f_tokens)\n",
    "basketball_words_count = Counter(b_tokens)\n",
    "football_words_total_count = len(f_tokens)\n",
    "basketball_words_total_count = len(b_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('leagu', 2.205488103743583), ('I', 1.2751488835691884), ('s', 1.259293332036864), ('year', 1.1757236671289504), ('player', 0.9657418580177081), ('say', 0.8070262750766966), ('play', 0.794789046616545), ('be', 0.7837811764435959), ('season', 0.6728599571040706), ('game', 0.41019653524962174)]\n[('game', 2.437855793665976), ('season', 1.486193359319391), ('be', 1.27586631327062), ('play', 1.2581954976066263), ('say', 1.2391170286307764), ('player', 1.0354733945700672), ('year', 0.8505399933318877), ('s', 0.7940961605685104), ('I', 0.7842221507507133), ('leagu', 0.4534143704074421)]\n"
     ]
    }
   ],
   "source": [
    "football_normalized_frequencies = {}\n",
    "basketball_normalized_frequencies = {}\n",
    "for word in top_ten_common_words:\n",
    "    word = word[0]\n",
    "    football_normalized_frequencies[word] = (football_words_count[word]/football_words_total_count)/(basketball_words_count[word]/basketball_words_total_count)\n",
    "    basketball_normalized_frequencies[word] = (basketball_words_count[word]/basketball_words_total_count)/(football_words_count[word]/football_words_total_count)\n",
    "\n",
    "print(sorted(football_normalized_frequencies.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "print(sorted(basketball_normalized_frequencies.items(), key=lambda x: x[1], reverse=True)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         00       000      0000   0000all    0000am    0000bc  0000blog  \\\n0  0.001619  0.000407  0.000618  0.000000  0.000033  0.000004  0.000011   \n1  0.001640  0.000060  0.000975  0.000009  0.000000  0.000000  0.000000   \n\n    0000bst  0000calcium   0000cet  ...      zych      zyci   zydruna  \\\n0  0.000014     0.000004  0.000004  ...  0.000004  0.000004  0.000000   \n1  0.000000     0.000000  0.000000  ...  0.000000  0.000000  0.000085   \n\n       zygi  zygimanta     zylan     zynex      zyro       zyz        zz  \n0  0.000000   0.000000  0.000000  0.000004  0.000011  0.000004  0.000004  \n1  0.000019   0.000009  0.000066  0.000000  0.000000  0.000000  0.000000  \n\n[2 rows x 68723 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([f_corpus, b_corpus])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "top 10 tf-idf football:\n      leagu      club    season        be    player      year       say  \\\n0  0.219325  0.202116  0.172411  0.170882  0.168717  0.161329  0.155355   \n\n      game      play        he  \n0  0.15407  0.147864  0.139387  \n********************************************************************************\ntop 10 tf-idf basketball:\n       game     point    season      team      nba        be       say  \\\n1  0.343101  0.235906  0.234065  0.219021  0.20225  0.199157  0.175846   \n\n       play    player       get  \n1  0.169944  0.159585  0.133551  \n"
     ]
    }
   ],
   "source": [
    "print(\"top 10 tf-idf football:\")\n",
    "print(df.sort_values(by=0,axis=1,ascending=False).iloc[:, : 10][:1])\n",
    "print(\"*\"*80)\n",
    "print(\"top 10 tf-idf basketball:\")\n",
    "print(df.sort_values(by=1,axis=1,ascending=False).iloc[:, : 10][1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram = Counter(words_union)\n",
    "histogram = sorted(histogram.items(), key=lambda pair: pair[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('s', 190032), ('game', 110895), ('season', 101799), ('leagu', 98717), ('be', 96012), ('player', 89284), ('say', 86512), ('play', 82724), ('I', 82416), ('year', 81320), ('club', 81082), ('team', 76518), ('get', 72828), ('he', 71397), ('go', 67812), ('make', 61870), ('one', 60968), ('last', 59526), ('point', 58272), ('first', 56440), ('back', 55793), ('time', 54207), ('we', 54099), ('goal', 51517), ('unit', 51357), ('good', 50333), ('win', 48256), ('two', 46909), ('come', 44937), ('see', 43837), ('premier', 43288), ('well', 42655), ('take', 42243), ('would', 41776), ('citi', 40365), ('like', 39787), ('think', 38861), ('also', 38616), ('old', 38265), ('it', 37899)]\n"
     ]
    }
   ],
   "source": [
    "print(histogram[:40])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}